"""API æœåŠ¡æ¨¡å—

æä¾› REST API ä¾›å‰ç«¯è°ƒç”¨
"""

from datetime import datetime, timedelta
from typing import Optional
from zoneinfo import ZoneInfo

from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
from loguru import logger
from pydantic import BaseModel, Field
from sqlalchemy import func, select

from src.config import settings
from src.storage import get_db_manager
from src.storage.models import Post, ScrapeLog
from src.runtime_config import (
    get_runtime_config,
    NotificationConfig,
    ScrapeConfig,
    TranslateConfig,
)

# æœåŠ¡å™¨æ—¶åŒº
SERVER_TZ = ZoneInfo(settings.timezone)

# ç¿»è¯‘å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_translator = None

def get_translator():
    """è·å–ç¿»è¯‘å™¨å®ä¾‹"""
    global _translator
    if _translator is None:
        runtime_config = get_runtime_config()
        if runtime_config.translate.translate_enabled and settings.translate_enabled:
            try:
                from src.integrations.translator import TencentTranslator
                _translator = TencentTranslator()
                if not _translator.enabled:
                    _translator = None
            except Exception as e:
                logger.warning(f"ç¿»è¯‘å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                _translator = None
    return _translator

def translate_post_content(post: Post, db) -> str:
    """ç¿»è¯‘å¸–å­å†…å®¹å¹¶æ›´æ–°æ•°æ®åº“
    
    Args:
        post: å¸–å­å¯¹è±¡
        db: æ•°æ®åº“ç®¡ç†å™¨
        
    Returns:
        ç¿»è¯‘åçš„å†…å®¹ï¼Œå¦‚æœç¿»è¯‘å¤±è´¥æˆ–ä¸éœ€è¦ç¿»è¯‘åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    # å¦‚æœå·²æœ‰ç¿»è¯‘ï¼Œç›´æ¥è¿”å›
    if post.translated_content:
        return post.translated_content
    
    translator = get_translator()
    if not translator:
        return ""
    
    content = post.content or ""
    if not content.strip():
        logger.debug(f"å¸–å­ {post.post_id} å†…å®¹ä¸ºç©ºï¼ˆå¯èƒ½æ˜¯è§†é¢‘/å›¾ç‰‡ï¼‰ï¼Œè·³è¿‡ç¿»è¯‘")
        return ""
    
    try:
        _, translated = translator.translate_if_english(content)
        if translated:
            # æ›´æ–°æ•°æ®åº“
            db.update_translation(post.id, translated)
            logger.debug(f"å¸–å­ {post.post_id} ç¿»è¯‘å®Œæˆ")
            return translated
    except Exception as e:
        logger.warning(f"ç¿»è¯‘å¸–å­ {post.post_id} å¤±è´¥: {e}")
    
    return ""

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="Trump Truth Social Monitor API",
    description="Trump Truth Social å¸–å­ç›‘æ§ API",
    version="1.0.0",
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# å“åº”æ¨¡å‹
class PostResponse(BaseModel):
    id: int
    post_id: str
    username: str
    content: Optional[str]
    url: Optional[str]
    reblogs_count: int
    favourites_count: int
    replies_count: int
    is_reblog: bool
    posted_at: Optional[datetime]
    created_at: datetime
    translated_content: Optional[str] = None
    translated_at: Optional[datetime] = None

    class Config:
        from_attributes = True


class StatsResponse(BaseModel):
    total_posts: int
    today_posts: int
    last_scrape: Optional[datetime]
    next_scrape: Optional[datetime]
    scrape_interval: int


class ConfigResponse(BaseModel):
    api_fetch_limit: int
    scrape_interval: int
    sleep_scrape_interval: int
    normal_scrape_interval: int


class PostsListResponse(BaseModel):
    posts: list[PostResponse]
    total: int
    page: int
    page_size: int


# API è·¯ç”±
@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "timestamp": datetime.now().isoformat()}


@app.get("/api/config", response_model=ConfigResponse)
async def get_config():
    """è·å–å‰ç«¯é…ç½®"""
    return ConfigResponse(
        api_fetch_limit=settings.api_fetch_limit,
        scrape_interval=settings.scrape_interval,
        sleep_scrape_interval=settings.sleep_scrape_interval,
        normal_scrape_interval=settings.normal_scrape_interval,
    )


@app.get("/api/stats", response_model=StatsResponse)
async def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    db = get_db_manager()
    
    with db.get_session() as session:
        # æ€»å¸–å­æ•°
        total_posts = session.execute(
            select(func.count(Post.id))
        ).scalar() or 0
        
        # ä»Šæ—¥å¸–å­æ•°ï¼ˆåŸºäºå¸–å­å‘å¸ƒæ—¶é—´ posted_atï¼Œè€Œéå…¥åº“æ—¶é—´ created_atï¼‰
        today_start = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        today_posts = session.execute(
            select(func.count(Post.id)).where(Post.posted_at >= today_start)
        ).scalar() or 0
        
        # æœ€åé‡‡é›†æ—¶é—´
        last_scrape_log = session.execute(
            select(ScrapeLog)
            .where(ScrapeLog.status == "success")
            .order_by(ScrapeLog.finished_at.desc())
            .limit(1)
        ).scalar_one_or_none()
        
        last_scrape = last_scrape_log.finished_at if last_scrape_log else None
        
        # è®¡ç®—ä¸‹æ¬¡é‡‡é›†æ—¶é—´
        next_scrape = None
        if last_scrape:
            next_scrape = last_scrape + timedelta(seconds=settings.scrape_interval)
    
    return StatsResponse(
        total_posts=total_posts,
        today_posts=today_posts,
        last_scrape=last_scrape,
        next_scrape=next_scrape,
        scrape_interval=settings.scrape_interval,
    )


@app.get("/api/posts", response_model=PostsListResponse)
async def get_posts(
    page: int = Query(1, ge=1, description="é¡µç "),
    limit: int = Query(20, ge=1, le=500, description="æ¯é¡µæ•°é‡"),
    filter_type: Optional[str] = Query(None, description="è¿‡æ»¤ç±»å‹: original/reblog"),
    search: Optional[str] = Query(None, description="æœç´¢å…³é”®è¯"),
):
    """è·å–å¸–å­åˆ—è¡¨"""
    db = get_db_manager()
    
    with db.get_session() as session:
        # æ„å»ºæŸ¥è¯¢
        query = select(Post).order_by(Post.posted_at.desc())
        count_query = select(func.count(Post.id))
        
        # è¿‡æ»¤ç±»å‹
        if filter_type == "original":
            query = query.where(Post.is_reblog == False)
            count_query = count_query.where(Post.is_reblog == False)
        elif filter_type == "reblog":
            query = query.where(Post.is_reblog == True)
            count_query = count_query.where(Post.is_reblog == True)
        
        # æœç´¢
        if search:
            search_pattern = f"%{search}%"
            query = query.where(Post.content.like(search_pattern))
            count_query = count_query.where(Post.content.like(search_pattern))
        
        # æ€»æ•°
        total = session.execute(count_query).scalar() or 0
        
        # åˆ†é¡µ
        offset = (page - 1) * limit
        query = query.offset(offset).limit(limit)
        
        posts = session.execute(query).scalars().all()
        
        # è½¬æ¢ä¸ºå“åº”æ¨¡å‹
        posts_data = [
            PostResponse(
                id=p.id,
                post_id=p.post_id,
                username=p.username,
                content=p.content,
                url=p.url,
                reblogs_count=p.reblogs_count,
                favourites_count=p.favourites_count,
                replies_count=p.replies_count,
                is_reblog=p.is_reblog,
                posted_at=p.posted_at,
                created_at=p.created_at,
                translated_content=p.translated_content,
                translated_at=p.translated_at,
            )
            for p in posts
        ]
    
    return PostsListResponse(
        posts=posts_data,
        total=total,
        page=page,
        page_size=limit,
    )


@app.get("/api/posts/{post_id}")
async def get_post(post_id: str):
    """è·å–å•ä¸ªå¸–å­è¯¦æƒ…"""
    db = get_db_manager()
    
    post = db.get_post_by_id(post_id)
    if not post:
        raise HTTPException(status_code=404, detail="Post not found")
    
    return PostResponse(
        id=post.id,
        post_id=post.post_id,
        username=post.username,
        content=post.content,
        url=post.url,
        reblogs_count=post.reblogs_count,
        favourites_count=post.favourites_count,
        replies_count=post.replies_count,
        is_reblog=post.is_reblog,
        posted_at=post.posted_at,
        created_at=post.created_at,
        translated_content=post.translated_content,
        translated_at=post.translated_at,
    )


@app.get("/api/scrape-logs")
async def get_scrape_logs(limit: int = Query(20, ge=1, le=100)):
    """è·å–é‡‡é›†æ—¥å¿—"""
    db = get_db_manager()
    
    with db.get_session() as session:
        logs = session.execute(
            select(ScrapeLog)
            .order_by(ScrapeLog.started_at.desc())
            .limit(limit)
        ).scalars().all()
        
        return [
            {
                "id": log.id,
                "username": log.username,
                "status": log.status,
                "total_fetched": log.total_fetched,
                "new_posts": log.new_posts,
                "updated_posts": log.updated_posts,
                "error_message": log.error_message,
                "started_at": log.started_at.isoformat() if log.started_at else None,
                "finished_at": log.finished_at.isoformat() if log.finished_at else None,
                "duration_seconds": log.duration_seconds,
            }
            for log in logs
        ]


# ==================== è®¾ç½® API ====================

class NotificationConfigRequest(BaseModel):
    """é€šçŸ¥é…ç½®è¯·æ±‚"""
    feishu_enabled: bool = True
    feishu_webhook: Optional[str] = None
    feishu_secret: Optional[str] = None
    realtime_enabled: bool = True
    daily_report_enabled: bool = True
    daily_report_time: str = "09:00"
    weekly_report_enabled: bool = True
    weekly_report_time: str = "09:00"
    weekly_report_day: int = Field(default=1, ge=1, le=7, description="1-7 å¯¹åº”å‘¨ä¸€åˆ°å‘¨æ—¥")
    # æŠ¥å‘Šæ˜¾ç¤ºé…ç½®
    full_display_count: int = Field(default=10, ge=3, le=20, description="å®Œæ•´æ˜¾ç¤ºå¸–å­æ•°é‡")
    summary_display_count: int = Field(default=10, ge=0, le=20, description="æ‘˜è¦æ˜¾ç¤ºå¸–å­æ•°é‡")
    ai_analysis_limit: int = Field(default=20, ge=5, le=50, description="AI åˆ†æå¸–å­æ•°é‡ä¸Šé™")
    # äº’åŠ¨é‡æƒé‡
    weight_replies: int = Field(default=3, ge=1, le=10, description="è¯„è®ºæƒé‡")
    weight_reblogs: int = Field(default=2, ge=1, le=10, description="è½¬å‘æƒé‡")
    weight_favourites: int = Field(default=1, ge=1, le=10, description="ç‚¹èµæƒé‡")


class ScrapeConfigRequest(BaseModel):
    """é‡‡é›†é…ç½®è¯·æ±‚"""
    scrape_enabled: bool = True
    normal_scrape_interval: int = Field(default=3600, ge=300, le=86400, description="æ­£å¸¸æ—¶æ®µé‡‡é›†é—´éš”ï¼ˆç§’ï¼‰")
    sleep_scrape_interval: int = Field(default=21600, ge=3600, le=86400, description="ç¡çœ æ—¶æ®µé‡‡é›†é—´éš”ï¼ˆç§’ï¼‰")
    min_scrape_gap: int = Field(default=300, ge=60, le=3600, description="æœ€å°é‡‡é›†é—´éš”ï¼ˆç§’ï¼‰")
    trump_sleep_start_hour: int = Field(default=0, ge=0, le=23, description="Trump ç¡çœ å¼€å§‹æ—¶é—´ï¼ˆç¾ä¸œï¼‰")
    trump_sleep_end_hour: int = Field(default=7, ge=0, le=23, description="Trump ç¡çœ ç»“æŸæ—¶é—´ï¼ˆç¾ä¸œï¼‰")


class TranslateConfigRequest(BaseModel):
    """ç¿»è¯‘é…ç½®è¯·æ±‚"""
    translate_enabled: bool = True


class SettingsResponse(BaseModel):
    """å®Œæ•´è®¾ç½®å“åº”"""
    notification: NotificationConfig
    scrape: ScrapeConfig
    translate: TranslateConfig


class TestNotificationRequest(BaseModel):
    """æµ‹è¯•é€šçŸ¥è¯·æ±‚"""
    webhook_url: str
    secret: Optional[str] = None


class PushReportRequest(BaseModel):
    """æ‰‹åŠ¨æ¨é€æŠ¥å‘Šè¯·æ±‚"""
    report_type: str = Field(..., pattern="^(daily|weekly|test)$", description="æŠ¥å‘Šç±»å‹: daily/weekly/test")


@app.get("/api/settings", response_model=SettingsResponse)
async def get_settings():
    """è·å–æ‰€æœ‰è®¾ç½®"""
    config_mgr = get_runtime_config()
    config_mgr.load_from_db()
    
    return SettingsResponse(
        notification=config_mgr.notification,
        scrape=config_mgr.scrape,
        translate=config_mgr.translate,
    )


@app.get("/api/settings/notification", response_model=NotificationConfig)
async def get_notification_settings():
    """è·å–é€šçŸ¥è®¾ç½®"""
    config_mgr = get_runtime_config()
    config_mgr.load_from_db()
    return config_mgr.notification


@app.put("/api/settings/notification", response_model=NotificationConfig)
async def update_notification_settings(config: NotificationConfigRequest):
    """æ›´æ–°é€šçŸ¥è®¾ç½®"""
    config_mgr = get_runtime_config()
    
    new_config = NotificationConfig(**config.model_dump())
    success = config_mgr.update_notification(new_config)
    
    if not success:
        raise HTTPException(status_code=500, detail="ä¿å­˜é…ç½®å¤±è´¥")
    
    logger.info(f"é€šçŸ¥é…ç½®å·²æ›´æ–°: realtime={config.realtime_enabled}, daily={config.daily_report_enabled}, weekly={config.weekly_report_enabled}")
    return new_config


@app.get("/api/settings/scrape", response_model=ScrapeConfig)
async def get_scrape_settings():
    """è·å–é‡‡é›†è®¾ç½®"""
    config_mgr = get_runtime_config()
    config_mgr.load_from_db()
    return config_mgr.scrape


@app.put("/api/settings/scrape", response_model=ScrapeConfig)
async def update_scrape_settings(config: ScrapeConfigRequest):
    """æ›´æ–°é‡‡é›†è®¾ç½®"""
    config_mgr = get_runtime_config()
    
    new_config = ScrapeConfig(**config.model_dump())
    success = config_mgr.update_scrape(new_config)
    
    if not success:
        raise HTTPException(status_code=500, detail="ä¿å­˜é…ç½®å¤±è´¥")
    
    logger.info(f"é‡‡é›†é…ç½®å·²æ›´æ–°: normal_interval={config.normal_scrape_interval}s, sleep_interval={config.sleep_scrape_interval}s")
    return new_config


@app.get("/api/settings/translate", response_model=TranslateConfig)
async def get_translate_settings():
    """è·å–ç¿»è¯‘è®¾ç½®"""
    config_mgr = get_runtime_config()
    config_mgr.load_from_db()
    return config_mgr.translate


@app.put("/api/settings/translate", response_model=TranslateConfig)
async def update_translate_settings(config: TranslateConfigRequest):
    """æ›´æ–°ç¿»è¯‘è®¾ç½®"""
    config_mgr = get_runtime_config()
    
    new_config = TranslateConfig(**config.model_dump())
    success = config_mgr.update_translate(new_config)
    
    if not success:
        raise HTTPException(status_code=500, detail="ä¿å­˜é…ç½®å¤±è´¥")
    
    logger.info(f"ç¿»è¯‘é…ç½®å·²æ›´æ–°: enabled={config.translate_enabled}")
    return new_config


@app.post("/api/settings/test-notification")
async def test_notification(request: TestNotificationRequest):
    """æµ‹è¯•é£ä¹¦é€šçŸ¥"""
    try:
        from src.notification import FeishuClient
        
        client = FeishuClient(
            webhook_url=request.webhook_url,
            secret=request.secret,
        )
        
        success = await client.send_text(
            "ğŸ”” Trump Monitor æµ‹è¯•é€šçŸ¥\n\n"
            "è¿™æ˜¯ä¸€æ¡æµ‹è¯•æ¶ˆæ¯ï¼Œç”¨äºéªŒè¯æ¨é€é…ç½®æ˜¯å¦æ­£ç¡®ã€‚\n"
            f"å¦‚æœæ‚¨æ”¶åˆ°æ­¤æ¶ˆæ¯ï¼Œè¯´æ˜é…ç½®æˆåŠŸï¼\n\n"
            f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        )
        
        if success:
            return {"success": True, "message": "æµ‹è¯•æ¶ˆæ¯å‘é€æˆåŠŸ"}
        else:
            raise HTTPException(status_code=500, detail="å‘é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥ Webhook URL")
            
    except Exception as e:
        logger.error(f"æµ‹è¯•é€šçŸ¥å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å‘é€æµ‹è¯•æ¶ˆæ¯å¤±è´¥: {str(e)}")


@app.post("/api/settings/push-report")
async def push_report(request: PushReportRequest):
    """æ‰‹åŠ¨æ¨é€æŠ¥å‘Š
    
    - daily: ä»Šæ—¥å¸–å­æ‘˜è¦
    - weekly: æœ¬å‘¨å¸–å­æ€»ç»“
    - test: æµ‹è¯•æ¨é€
    """
    config_mgr = get_runtime_config()
    config_mgr.load_from_db()
    
    notification_config = config_mgr.notification
    if not notification_config.feishu_enabled or not notification_config.feishu_webhook:
        raise HTTPException(status_code=400, detail="æœªé…ç½®é£ä¹¦é€šçŸ¥ï¼Œè¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½® Webhook URL")
    
    try:
        from src.notification import FeishuClient
        from src.storage import get_db_manager
        from src.config import settings
        from src.analyzer import get_trump_analyzer
        from sqlalchemy import func, select, and_
        
        client = FeishuClient(
            webhook_url=notification_config.feishu_webhook,
            secret=notification_config.feishu_secret,
        )
        db = get_db_manager()
        
        # è·å– AI åˆ†æå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        trump_analyzer = None
        if settings.knot_enabled:
            try:
                trump_analyzer = get_trump_analyzer()
            except Exception as e:
                logger.warning(f"AI åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        
        if request.report_type == "test":
            success = await client.send_text(
                "ğŸ”” Trump Monitor æ‰‹åŠ¨æ¨é€æµ‹è¯•\n\n"
                f"æ¨é€æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            )
            return {"success": success, "message": "æµ‹è¯•æ¨é€å®Œæˆ" if success else "æ¨é€å¤±è´¥"}
        
        elif request.report_type == "daily":
            # è·å–è¿‡å»24å°æ—¶å¸–å­ï¼ˆä½¿ç”¨æœåŠ¡å™¨æ—¶åŒºï¼‰
            now = datetime.now(SERVER_TZ)
            time_24h_ago = now - timedelta(hours=24)
            time_24h_ago_naive = time_24h_ago.replace(tzinfo=None)
            logger.info(f"æ—¥æŠ¥æ—¶é—´èŒƒå›´: è¿‡å»24å°æ—¶ ({time_24h_ago_naive} è‡³ä»Š, {settings.timezone})")
            
            def _has_text_content(post) -> bool:
                """åˆ¤æ–­å¸–å­æ˜¯å¦æœ‰æ–‡æœ¬å†…å®¹"""
                content = post.content if hasattr(post, 'content') else post.get('content', '')
                return bool(content and content.strip())
            
            with db.get_session() as session:
                posts = session.execute(
                    select(Post)
                    .where(Post.posted_at >= time_24h_ago_naive)
                    .order_by(Post.posted_at.desc())
                ).scalars().all()
                
                if not posts:
                    return {"success": True, "message": "è¿‡å»24å°æ—¶æš‚æ— æ–°å¸–å­"}
                
                # ç»Ÿè®¡æœ‰å†…å®¹å’Œæ— å†…å®¹çš„å¸–å­
                text_posts_count = sum(1 for p in posts if _has_text_content(p))
                media_posts_count = len(posts) - text_posts_count
                logger.info(f"å¸–å­ç»Ÿè®¡: æ–‡æœ¬ {text_posts_count} æ¡ï¼Œåª’ä½“ {media_posts_count} æ¡")
                
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼Œå¹¶è¡¥å……ç¿»è¯‘
                posts_data = []
                for post in posts:
                    # å¦‚æœæ²¡æœ‰ç¿»è¯‘ï¼Œå°è¯•ç¿»è¯‘
                    translated = post.translated_content or translate_post_content(post, db)
                    posts_data.append({
                        "content": post.content or "",
                        "translated_content": translated,
                        "posted_at": post.posted_at,
                        "is_reblog": post.is_reblog,
                        "url": post.url or "",
                    })
                
                # AI åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰- åªåˆ†ææœ‰æ–‡æœ¬å†…å®¹çš„å¸–å­
                ai_analysis = None
                if trump_analyzer and text_posts_count > 0:
                    try:
                        # è¿‡æ»¤å‡ºæœ‰æ–‡æœ¬å†…å®¹çš„å¸–å­
                        posts_for_analysis = [
                            {
                                "content": p.get("content", ""),
                                "translated_content": p.get("translated_content", ""),
                                "posted_at": p["posted_at"].isoformat() if hasattr(p.get("posted_at"), 'isoformat') else str(p.get("posted_at", "")),
                            }
                            for p in posts_data
                            if p.get("content", "").strip()  # åªåˆ†ææœ‰å†…å®¹çš„
                        ]
                        if posts_for_analysis:
                            logger.info(f"å¼€å§‹ AI åˆ†ææ—¥æŠ¥ ({len(posts_for_analysis)} æ¡æœ‰æ–‡æœ¬å¸–å­)...")
                            result = await trump_analyzer.analyze_batch(
                                posts=posts_for_analysis,
                                analysis_focus="daily_summary"
                            )
                            if result["status"] == "success" and result["analysis"]:
                                ai_analysis = result["analysis"]
                                logger.info("æ—¥æŠ¥ AI åˆ†æå®Œæˆ")
                    except Exception as e:
                        logger.warning(f"æ—¥æŠ¥ AI åˆ†æå¤±è´¥: {e}")
                elif text_posts_count == 0:
                    logger.info("æ— æ–‡æœ¬å¸–å­ï¼Œè·³è¿‡ AI åˆ†æ")
                
                success = await client.send_daily_report(
                    posts_data, 
                    time_24h_ago_naive, 
                    ai_analysis=ai_analysis,
                    text_posts_count=text_posts_count,
                    media_posts_count=media_posts_count,
                )
                return {"success": success, "message": f"æ—¥æŠ¥æ¨é€å®Œæˆï¼Œå…± {len(posts)} æ¡å¸–å­ï¼ˆæ–‡æœ¬ {text_posts_count}ï¼Œåª’ä½“ {media_posts_count}ï¼‰"}
        
        elif request.report_type == "weekly":
            # è·å–è¿‡å»7å¤©å¸–å­ï¼ˆä½¿ç”¨æœåŠ¡å™¨æ—¶åŒºï¼‰
            now = datetime.now(SERVER_TZ)
            time_7d_ago = now - timedelta(days=7)
            time_7d_ago_naive = time_7d_ago.replace(tzinfo=None)
            logger.info(f"å‘¨æŠ¥æ—¶é—´èŒƒå›´: è¿‡å»7å¤© ({time_7d_ago_naive} è‡³ä»Š, {settings.timezone})")
            
            # ä»é…ç½®è·å–æ˜¾ç¤ºå’Œåˆ†æå‚æ•°
            full_display_count = notification_config.full_display_count
            summary_display_count = notification_config.summary_display_count
            ai_analysis_limit = notification_config.ai_analysis_limit
            weight_replies = notification_config.weight_replies
            weight_reblogs = notification_config.weight_reblogs
            weight_favourites = notification_config.weight_favourites
            
            def _has_text_content(post) -> bool:
                """åˆ¤æ–­å¸–å­æ˜¯å¦æœ‰æ–‡æœ¬å†…å®¹"""
                content = post.content if hasattr(post, 'content') else post.get('content', '')
                return bool(content and content.strip())
            
            with db.get_session() as session:
                # è·å–æ‰€æœ‰å¸–å­ç”¨äºç»Ÿè®¡
                all_posts = session.execute(
                    select(Post).where(Post.posted_at >= time_7d_ago_naive)
                ).scalars().all()
                
                total_count = len(all_posts)
                if total_count == 0:
                    return {"success": True, "message": "è¿‡å»7å¤©æš‚æ— å¸–å­"}
                
                # ç»Ÿè®¡åˆ†ç±»
                text_posts = [p for p in all_posts if _has_text_content(p)]
                media_posts = [p for p in all_posts if not _has_text_content(p)]
                text_posts_count = len(text_posts)
                media_posts_count = len(media_posts)
                
                original_count = sum(1 for p in all_posts if not p.is_reblog)
                reblog_count = total_count - original_count
                
                logger.info(f"å‘¨æŠ¥ç»Ÿè®¡: æ€» {total_count} æ¡ï¼Œæ–‡æœ¬ {text_posts_count}ï¼Œåª’ä½“ {media_posts_count}ï¼ŒåŸåˆ› {original_count}ï¼Œè½¬å‘ {reblog_count}")
                
                # æŒ‰åŠ æƒäº’åŠ¨é‡æ’åºï¼ˆåªç»Ÿè®¡æœ‰æ–‡æœ¬å†…å®¹çš„å¸–å­ï¼‰
                def calc_weighted_score(post) -> int:
                    return (
                        post.replies_count * weight_replies +
                        post.reblogs_count * weight_reblogs +
                        post.favourites_count * weight_favourites
                    )
                
                sorted_text_posts = sorted(text_posts, key=calc_weighted_score, reverse=True)
                
                # å–çƒ­é—¨å¸–å­ç”¨äºæ˜¾ç¤ºå’Œåˆ†æ
                hot_posts_for_display = sorted_text_posts[:full_display_count + summary_display_count]
                hot_posts_for_ai = sorted_text_posts[:ai_analysis_limit]
                
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                hot_posts_data = []
                for post in hot_posts_for_display:
                    translated = post.translated_content or translate_post_content(post, db)
                    hot_posts_data.append({
                        "content": post.content or "",
                        "translated_content": translated,
                        "reblogs_count": post.reblogs_count,
                        "favourites_count": post.favourites_count,
                        "replies_count": post.replies_count,
                        "weighted_score": calc_weighted_score(post),
                        "url": post.url or "",
                        "posted_at": post.posted_at.isoformat() if post.posted_at else None,
                    })
                
                # AI åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰- åªåˆ†ææœ‰æ–‡æœ¬å†…å®¹çš„çƒ­é—¨å¸–å­
                ai_analysis = None
                if trump_analyzer and hot_posts_for_ai:
                    try:
                        logger.info(f"å¼€å§‹ AI åˆ†æå‘¨æŠ¥ ({len(hot_posts_for_ai)} æ¡æœ‰æ–‡æœ¬çƒ­é—¨å¸–å­)...")
                        posts_for_analysis = [
                            {
                                "content": p.content or "",
                                "translated_content": p.translated_content or "",
                                "posted_at": p.posted_at.isoformat() if p.posted_at else "",
                            }
                            for p in hot_posts_for_ai
                            if p.content and p.content.strip()
                        ]
                        if posts_for_analysis:
                            result = await trump_analyzer.analyze_batch(
                                posts=posts_for_analysis,
                                analysis_focus="weekly_summary"
                            )
                            if result["status"] == "success" and result["analysis"]:
                                ai_analysis = result["analysis"]
                                logger.info("å‘¨æŠ¥ AI åˆ†æå®Œæˆ")
                    except Exception as e:
                        logger.warning(f"å‘¨æŠ¥ AI åˆ†æå¤±è´¥: {e}")
                
                # è®¡ç®—å‰©ä½™å¸–å­æ•°
                remaining_count = max(0, text_posts_count - full_display_count - summary_display_count)
                
                success = await client.send_weekly_report(
                    week_start=time_7d_ago_naive,
                    week_end=now.replace(tzinfo=None),
                    total_posts=total_count,
                    original_posts=original_count,
                    reblog_posts=reblog_count,
                    hot_posts=hot_posts_data,
                    ai_analysis=ai_analysis,
                    full_display_count=full_display_count,
                    summary_display_count=summary_display_count,
                    text_posts_count=text_posts_count,
                    media_posts_count=media_posts_count,
                    remaining_count=remaining_count,
                )
                return {"success": success, "message": f"å‘¨æŠ¥æ¨é€å®Œæˆï¼Œè¿‡å»7å¤©å…± {total_count} æ¡å¸–å­ï¼ˆæ–‡æœ¬ {text_posts_count}ï¼Œåª’ä½“ {media_posts_count}ï¼‰"}
        
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æŠ¥å‘Šç±»å‹")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ¨é€æŠ¥å‘Šå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¨é€æŠ¥å‘Šå¤±è´¥: {str(e)}")


# ==================== AI åˆ†æ API ====================


class AnalyzePostRequest(BaseModel):
    """AI åˆ†æè¯·æ±‚"""
    content: str = Field(..., description="å¸–å­åŸæ–‡ï¼ˆè‹±æ–‡ï¼‰")
    translated_content: Optional[str] = Field(None, description="ç¿»è¯‘å†…å®¹ï¼ˆä¸­æ–‡ï¼‰")
    posted_at: Optional[str] = Field(None, description="å‘å¸ƒæ—¶é—´ï¼ˆISO æ ¼å¼ï¼‰")
    context: Optional[str] = Field(None, description="è¡¥å……èƒŒæ™¯ä¿¡æ¯")


@app.post("/api/analyze")
async def analyze_post(request: AnalyzePostRequest):
    """AI åˆ†æå•æ¡å¸–å­
    
    ä½¿ç”¨ Trump è¨€è®ºåˆ†æ Agent åˆ†æå¸–å­å¯¹èµ„æœ¬å¸‚åœºã€æŠ•èµ„è¶‹åŠ¿ã€ä¸–ç•Œå±€åŠ¿çš„å½±å“ã€‚
    
    éœ€è¦é…ç½®ï¼š
    - KNOT_ENABLED=true
    - KNOT_AGENT_ID=<æ™ºèƒ½ä½“ID>
    - KNOT_API_TOKEN=<ç”¨æˆ·Token> æˆ– KNOT_AGENT_TOKEN + KNOT_USERNAME
    """
    from src.config import settings
    from src.analyzer import get_trump_analyzer
    
    if not settings.knot_enabled:
        raise HTTPException(
            status_code=400, 
            detail="AI åˆ†ææœªå¯ç”¨ï¼Œè¯·åœ¨ .env ä¸­è®¾ç½® KNOT_ENABLED=true å¹¶é…ç½®ç›¸å…³å‚æ•°"
        )
    
    try:
        analyzer = get_trump_analyzer()
        
        # è§£æå‘å¸ƒæ—¶é—´
        posted_at = None
        if request.posted_at:
            try:
                posted_at = datetime.fromisoformat(request.posted_at.replace('Z', '+00:00'))
            except ValueError:
                pass
        
        result = await analyzer.analyze_post(
            content=request.content,
            translated_content=request.translated_content,
            posted_at=posted_at,
            context=request.context,
        )
        
        if result["status"] == "error":
            raise HTTPException(status_code=500, detail=result["error"])
        
        return {
            "success": True,
            "status": result["status"],
            "analysis": result["analysis"],
            "analyzed_at": result["analyzed_at"],
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"AI åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")


@app.post("/api/analyze/{post_id}")
async def analyze_post_by_id(post_id: int):
    """æ ¹æ®å¸–å­ ID è¿›è¡Œ AI åˆ†æ"""
    from src.config import settings
    from src.analyzer import get_trump_analyzer
    from src.storage import get_db_manager
    
    if not settings.knot_enabled:
        raise HTTPException(
            status_code=400, 
            detail="AI åˆ†ææœªå¯ç”¨ï¼Œè¯·åœ¨ .env ä¸­è®¾ç½® KNOT_ENABLED=true å¹¶é…ç½®ç›¸å…³å‚æ•°"
        )
    
    db = get_db_manager()
    
    # è·å–å¸–å­
    with db.get_session() as session:
        post = session.execute(
            select(Post).where(Post.id == post_id)
        ).scalar_one_or_none()
        
        if not post:
            raise HTTPException(status_code=404, detail="å¸–å­ä¸å­˜åœ¨")
        
        content = post.content
        translated_content = post.translated_content
        posted_at = post.posted_at
    
    try:
        analyzer = get_trump_analyzer()
        
        result = await analyzer.analyze_post(
            content=content or "",
            translated_content=translated_content,
            posted_at=posted_at,
        )
        
        if result["status"] == "error":
            raise HTTPException(status_code=500, detail=result["error"])
        
        return {
            "success": True,
            "post_id": post_id,
            "status": result["status"],
            "analysis": result["analysis"],
            "analyzed_at": result["analyzed_at"],
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"AI åˆ†æå¸–å­ {post_id} å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")


@app.get("/api/analyze/status")
async def get_analyze_status():
    """è·å– AI åˆ†ææœåŠ¡çŠ¶æ€"""
    from src.config import settings
    
    status = {
        "enabled": settings.knot_enabled,
        "agent_id": settings.knot_agent_id if settings.knot_enabled else None,
        "model": settings.knot_model if settings.knot_enabled else None,
        "auth_mode": None,
    }
    
    if settings.knot_enabled:
        if settings.knot_api_token:
            status["auth_mode"] = "api_token"
        elif settings.knot_agent_token:
            status["auth_mode"] = "agent_token"
    
    return status


# é™æ€æ–‡ä»¶æœåŠ¡ï¼ˆå‰ç«¯ï¼‰
# æ³¨æ„ï¼šè¿™ä¸ªéœ€è¦æ”¾åœ¨æœ€åï¼Œå¦åˆ™ä¼šæ‹¦æˆª API è·¯ç”±
@app.get("/")
async def serve_frontend():
    """æœåŠ¡å‰ç«¯é¡µé¢"""
    return FileResponse("frontend/index.html")


# æŒ‚è½½ JS å’Œ CSS é™æ€æ–‡ä»¶ç›®å½•
app.mount("/js", StaticFiles(directory="frontend/js"), name="js")
app.mount("/css", StaticFiles(directory="frontend/css"), name="css")


def run_api_server():
    """è¿è¡Œ API æœåŠ¡å™¨"""
    import uvicorn
    
    logger.info("å¯åŠ¨ API æœåŠ¡å™¨...")
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=6001,
        log_level="info",
    )


if __name__ == "__main__":
    run_api_server()
